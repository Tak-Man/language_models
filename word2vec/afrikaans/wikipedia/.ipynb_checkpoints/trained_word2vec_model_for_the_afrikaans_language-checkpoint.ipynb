{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained word2vec Model for the Afrikaans Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to improve language modeling for Afrikaans language tasks, a language model was trained using word2vec. The modeling of language tasks are known to be sensitive to the domain of usage - a language model for legal documents would not be expected to perform well on a lnaguage task involving a different domain (like technology) - so as a first attempt a 'general purpose' Afrikaan language model was trained using Afrikaans conten from wikepedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model should appear in the same directory as this document, with the name 'word2vec_af_wikipedia_500000.kv'. The .kv extension is from word2vec and indicates stored word vectors that are keyed by lookup tokens. By storing keyed vectors instead of the full model, the model would not be able to trained further. The advantage, though, is that the saved model file will be smaller in size. This model is 61MB's in size and git hub has a recommendation of a maximum file size of 50MB.\n",
    "\n",
    "The model can be used in the following way:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-3.8.3-cp36-cp36m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gensim) (1.19.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.1.2-py3-none-any.whl (111 kB)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gensim) (1.5.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-4.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\carlo\\appdata\\local\\programs\\python\\python36\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word vectors\n",
    "import gensim\n",
    "model_word_vectors = gensim.models.KeyedVectors.load(\"word2vec_af_wikipedia_500000.kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Word2Vec model was computed using the following parameters:\n",
    "- vector_size = 100, The number of dimensions of the embeddings\n",
    "- window = 5, The maximum distance between a target word and words around the target word\n",
    "- min_count = 500, The minimum count of words to consider when training the model\n",
    "\n",
    "These parameters were choosen arbitrarily, and different parameters will produce a different model. To assess which parameters are optimal a specific use case will be required. At the time of creating this model, no specific use case was identified and tests were still being devised to assess the performance of the model.\n",
    "\n",
    "It was run on 500,000 web pages retrieved from wikepedia, specifically for Afrikaans content. Minimal process was doen to the content of the web pages. the webpages were retreaved during December 2020 and January 2021.\n",
    "\n",
    "It took 35 hours to process. Most of that time was processing the html efrom the web pages. Only 3 hours of that was for calculating the Word2Vec model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.9409714 ,  4.496691  ,  6.708669  , -0.73155135,  1.3737465 ,\n",
       "       -1.5263249 ,  3.3370247 , -4.6911836 ,  2.3992188 ,  3.6628628 ,\n",
       "       -2.0540934 ,  4.4699187 , -0.831507  , -6.2117314 , -0.9083498 ,\n",
       "        3.5368695 , -0.14083028, -1.5410953 , -4.560557  ,  9.646817  ,\n",
       "       -0.47829846,  0.57315624, 10.241089  , -2.1930175 ,  4.319505  ,\n",
       "       -2.8107812 , -1.0864744 ,  1.9947112 , -1.4881661 , -1.1401428 ,\n",
       "        9.81603   ,  5.0182805 ,  3.5927055 , -6.8963404 ,  7.9481792 ,\n",
       "        3.4396493 ,  0.7054076 ,  0.49245477,  0.9315639 ,  2.610291  ,\n",
       "       -3.3183303 , -2.5655458 , -3.008428  , -1.5372294 , -2.4686952 ,\n",
       "        1.6182895 , -5.2672544 , -0.30342856, -3.9773808 ,  1.6618026 ,\n",
       "       -6.450713  , -1.3467321 , -1.3227992 ,  9.374692  , -1.404849  ,\n",
       "       10.4987335 , -2.2145247 ,  0.48969543, -2.9401553 ,  3.302072  ,\n",
       "       -0.516358  ,  7.0161767 ,  0.6384455 ,  2.995517  ,  7.2274675 ,\n",
       "       -5.422364  , -2.1316519 ,  0.35439172,  2.482815  ,  2.9416265 ,\n",
       "       -0.96180123,  4.675279  ,  8.01387   , -2.710501  ,  6.247717  ,\n",
       "       -0.52767706, -9.521625  , -3.86318   , -0.76059407, -4.3636856 ,\n",
       "       -1.0998063 ,  0.02362877,  3.7429335 , -4.1164746 , -2.5133483 ,\n",
       "       -2.1150804 ,  8.850275  ,  5.1679673 , -3.4448924 ,  0.61113036,\n",
       "       -0.29483086,  4.0508127 ,  2.7536478 ,  5.079633  , -3.2830954 ,\n",
       "        3.4864702 , -1.3478746 ,  7.1309714 ,  8.029671  ,  4.2433076 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the word 'vrou' we\n",
    "# look at one of the vectors (embeddings), it can be seen that it is indeed 100 dimensional.\n",
    "test_vector = model_word_vectors[\"vrou\"]\n",
    "test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test another word to see if it is present in the model\n",
    "len(model_word_vectors[\"seun\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test another word\n",
    "len(model_word_vectors[\"ontbyt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a particular word is not part of the model, a KeyError will be throw. For example, 'KeyError: \"word 'Donald Trump' not in vocabulary\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can be done with the word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 'seun' staan tot 'meisie', soos 'broer' staan tot <<'vrou'- 0.5705>> EXPECTED 'suster'\n",
      ">> 'pa' staan tot 'ma', soos 'vader' staan tot <<'moeder'- 0.6940>> EXPECTED 'moeder'\n",
      ">> 'vader' staan tot 'moeder', soos 'broers' staan tot <<'dogters'- 0.6842>> EXPECTED 'susters'\n",
      ">> 'gelukkig' staan tot 'ongelukkig', soos 'eerlik' staan tot <<'handeling'- 0.4491>> EXPECTED 'oneerlik'\n",
      "\n",
      ">> Similarity between 'vrou' and 'man' is 0.5491\n",
      ">> Similarity between 'broer' and 'seun' is 0.7525\n",
      ">> Similarity between 'pa' and 'man' is 0.4007\n",
      ">> Similarity between 'bewus' and 'onbewus' is 0.5069\n",
      "\n",
      ">> Most similar to 'oom'': [('broer', 0.7135692238807678), ('dogter', 0.6887587904930115), ('vader', 0.6787451505661011)]\n",
      ">> Most similar to 'tannie'': [('groothertogin', 0.5186679363250732), ('Nijinsky', 0.48672914505004883), ('Petrowna', 0.481914758682251)]\n",
      ">> Most similar to 'oupa'': [('pa', 0.6433155536651611), ('broer', 0.6229803562164307), ('tante', 0.6226500272750854)]\n",
      ">> Most similar to 'seun'': [('dogter', 0.8658730983734131), ('neef', 0.7544885873794556), ('broer', 0.752475380897522)]\n"
     ]
    }
   ],
   "source": [
    "# Family\n",
    "# seun meisie broer suster\n",
    "a = \"seun\"\n",
    "b = \"meisie\"\n",
    "c = \"broer\"\n",
    "d = \"suster\"\n",
    "result = model_word_vectors.most_similar(positive=[b, c], negative=[a])\n",
    "most_similar_key, similarity = result[0]\n",
    "print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {:.4f}>> EXPECTED '{}'\" \\\n",
    "      .format(a, b, c, most_similar_key, similarity, d))\n",
    "\n",
    "# Family\n",
    "# pa ma vader moeder\n",
    "a = \"pa\"\n",
    "b = \"ma\"\n",
    "c = \"vader\"\n",
    "d = \"moeder\"\n",
    "result = model_word_vectors.most_similar(positive=[b, c], negative=[a])\n",
    "most_similar_key, similarity = result[0]\n",
    "print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {:.4f}>> EXPECTED '{}'\" \\\n",
    "      .format(a, b, c, most_similar_key, similarity, d))\n",
    "\n",
    "# Plural\n",
    "# vader moeder broers susters\n",
    "a = \"vader\"\n",
    "b = \"moeder\"\n",
    "c = \"broers\"\n",
    "d = \"susters\"\n",
    "result = model_word_vectors.most_similar(positive=[b, c], negative=[a])\n",
    "most_similar_key, similarity = result[0]\n",
    "print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {:.4f}>> EXPECTED '{}'\" \\\n",
    "      .format(a, b, c, most_similar_key, similarity, d))\n",
    "\n",
    "# Opposites\n",
    "# gelukkig ongelukkig eerlik oneerlik\n",
    "a = \"gelukkig\"\n",
    "b = \"ongelukkig\"\n",
    "c = \"eerlik\"\n",
    "d = \"oneerlik\"\n",
    "result = model_word_vectors.most_similar(positive=[b, c], negative=[a])\n",
    "most_similar_key, similarity = result[0]\n",
    "print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {:.4f}>> EXPECTED '{}'\" \\\n",
    "      .format(a, b, c, most_similar_key, similarity, d))\n",
    "\n",
    "print()\n",
    "\n",
    "a = \"vrou\"\n",
    "b = \"man\"\n",
    "similarity = model_word_vectors.similarity(a, b)\n",
    "print(\">> Similarity between '{}' and '{}' is {:.4f}\".format(a, b, similarity))\n",
    "\n",
    "a = \"broer\"\n",
    "b = \"seun\"\n",
    "similarity = model_word_vectors.similarity(a, b)\n",
    "print(\">> Similarity between '{}' and '{}' is {:.4f}\".format(a, b, similarity))\n",
    "\n",
    "a = \"pa\"\n",
    "b = \"man\"\n",
    "similarity = model_word_vectors.similarity(a, b)\n",
    "print(\">> Similarity between '{}' and '{}' is {:.4f}\".format(a, b, similarity))\n",
    "\n",
    "a = \"bewus\"\n",
    "b = \"onbewus\"\n",
    "similarity = model_word_vectors.similarity(a, b)\n",
    "print(\">> Similarity between '{}' and '{}' is {:.4f}\".format(a, b, similarity))\n",
    "\n",
    "print()\n",
    "\n",
    "a = \"oom\"\n",
    "similar = model_word_vectors.similar_by_word(a)\n",
    "print(\">> Most similar to '{}'': {}\".format(a, similar[:3]))\n",
    "\n",
    "a = \"tannie\"\n",
    "similar = model_word_vectors.similar_by_word(a)\n",
    "print(\">> Most similar to '{}'': {}\".format(a, similar[:3]))\n",
    "\n",
    "a = \"oupa\"\n",
    "similar = model_word_vectors.similar_by_word(a)\n",
    "print(\">> Most similar to '{}'': {}\".format(a, similar[:3]))\n",
    "\n",
    "a = \"seun\"\n",
    "similar = model_word_vectors.similar_by_word(a)\n",
    "print(\">> Most similar to '{}'': {}\".format(a, similar[:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
