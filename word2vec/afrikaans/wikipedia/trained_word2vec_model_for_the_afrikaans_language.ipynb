{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained word2vec Model for the Afrikaans Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to improve language modeling for Afrikaans language tasks, a language model was trained using word2vec. The modeling of language tasks are known to be sensitive to the domain of usage - a language model for legal documents would not be expected to perform well on a lnaguage task involving a different domain (like technology) - so as a first attempt a 'general purpose' Afrikaan language model was trained using Afrikaans conten from wikepedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model should appear in the same directory as this document, with the name 'word2vec_af_wikipedia_500000.kv'. The .kv extension is from word2vec and indicates stored word vectors that are keyed by lookup tokens. By storing keyed vectors instead of the full model, the model would not be able to trained further. The advantage, though, is that the saved model file will be smaller in size. This model is 61MB's in size and git hub has a recommendation of a maximum file size of 50MB.\n",
    "\n",
    "The model can be used in the following way:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word vectors\n",
    "import gensim\n",
    "model_word_vectors = gensim.models.KeyedVectors.load(\"word2vec_af_wikipedia_500000_2021_01_07.kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Word2Vec model was computed using the following parameters:\n",
    "- vector_size = 100, The number of dimensions of the embeddings\n",
    "- window = 5, The maximum distance between a target word and words around the target word\n",
    "- min_count = 500, The minimum count of words to consider when training the model\n",
    "\n",
    "These parameters were choosen arbitrarily, and different parameters will produce a different model. To assess which parameters are optimal a specific use case will be required. At the time of creating this model, no specific use case was identified and tests were still being devised to assess the performance of the model.\n",
    "\n",
    "It was run on 500,000 web pages retrieved from wikepedia, specifically for Afrikaans content. Minimal process was doen to the content of the web pages. the webpages were retreaved during December 2020 and January 2021.\n",
    "\n",
    "It took 35 hours to process. Most of that time was processing the html efrom the web pages. Only 3 hours of that was for calculating the Word2Vec model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.9409714 ,  4.496691  ,  6.708669  , -0.73155135,  1.3737465 ,\n",
       "       -1.5263249 ,  3.3370247 , -4.6911836 ,  2.3992188 ,  3.6628628 ,\n",
       "       -2.0540934 ,  4.4699187 , -0.831507  , -6.2117314 , -0.9083498 ,\n",
       "        3.5368695 , -0.14083028, -1.5410953 , -4.560557  ,  9.646817  ,\n",
       "       -0.47829846,  0.57315624, 10.241089  , -2.1930175 ,  4.319505  ,\n",
       "       -2.8107812 , -1.0864744 ,  1.9947112 , -1.4881661 , -1.1401428 ,\n",
       "        9.81603   ,  5.0182805 ,  3.5927055 , -6.8963404 ,  7.9481792 ,\n",
       "        3.4396493 ,  0.7054076 ,  0.49245477,  0.9315639 ,  2.610291  ,\n",
       "       -3.3183303 , -2.5655458 , -3.008428  , -1.5372294 , -2.4686952 ,\n",
       "        1.6182895 , -5.2672544 , -0.30342856, -3.9773808 ,  1.6618026 ,\n",
       "       -6.450713  , -1.3467321 , -1.3227992 ,  9.374692  , -1.404849  ,\n",
       "       10.4987335 , -2.2145247 ,  0.48969543, -2.9401553 ,  3.302072  ,\n",
       "       -0.516358  ,  7.0161767 ,  0.6384455 ,  2.995517  ,  7.2274675 ,\n",
       "       -5.422364  , -2.1316519 ,  0.35439172,  2.482815  ,  2.9416265 ,\n",
       "       -0.96180123,  4.675279  ,  8.01387   , -2.710501  ,  6.247717  ,\n",
       "       -0.52767706, -9.521625  , -3.86318   , -0.76059407, -4.3636856 ,\n",
       "       -1.0998063 ,  0.02362877,  3.7429335 , -4.1164746 , -2.5133483 ,\n",
       "       -2.1150804 ,  8.850275  ,  5.1679673 , -3.4448924 ,  0.61113036,\n",
       "       -0.29483086,  4.0508127 ,  2.7536478 ,  5.079633  , -3.2830954 ,\n",
       "        3.4864702 , -1.3478746 ,  7.1309714 ,  8.029671  ,  4.2433076 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the word 'vrou' we\n",
    "# look at one of the vectors (embeddings), it can be seen that it is indeed 100 dimensional.\n",
    "test_vector = model_word_vectors[\"vrou\"]\n",
    "test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test another word to see if it is present in the model\n",
    "len(model_word_vectors[\"seun\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test another word\n",
    "len(model_word_vectors[\"ontbyt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.77464277,  5.8762918 , -5.731501  , -3.6681118 , -4.011173  ,\n",
       "       -9.028599  , -0.74995923, -0.8948402 , -0.31283137, -4.871647  ,\n",
       "        3.034864  , -1.2406137 ,  7.918766  , 10.318445  , -4.517135  ,\n",
       "       -9.164283  ,  0.38009557, -6.2928634 , -2.3856127 ,  7.126466  ,\n",
       "       -6.122153  , -2.092636  , -5.2584124 ,  6.7173157 ,  3.5572379 ,\n",
       "       12.739954  , -1.0829413 ,  1.7337722 ,  8.25426   ,  2.2497866 ,\n",
       "       -5.311122  , -1.7938011 , -0.13378747, -6.582857  , -0.77637875,\n",
       "        3.3784635 , 14.4127035 ,  3.6457427 , -2.245472  , -4.0668936 ,\n",
       "       -3.8872232 , -4.1480355 ,  1.3257781 ,  0.9594052 , -4.2537293 ,\n",
       "        4.333547  ,  0.96040857,  6.758412  , -2.2232096 , -0.6627186 ,\n",
       "       -0.5521825 , -3.6668394 ,  5.2618895 , -2.8118012 ,  1.2557592 ,\n",
       "        7.189046  , -0.835054  ,  0.38913158, -2.6705868 , -5.9174643 ,\n",
       "       -6.101463  , 10.014037  ,  1.1932181 , -1.8210099 , -6.3528605 ,\n",
       "        4.119733  ,  5.636462  , -1.952771  ,  2.5994592 ,  3.438037  ,\n",
       "        1.340022  ,  2.5789974 , -6.7715974 ,  3.3752153 ,  1.266045  ,\n",
       "       -0.36400053,  3.0254927 ,  2.4543083 ,  2.5999115 ,  2.8380387 ,\n",
       "       -4.685031  ,  0.65935594,  5.9687147 ,  4.9154305 , -0.23784609,\n",
       "       -1.8366741 ,  1.6819891 , -7.1569786 , -5.9805703 , -1.0335487 ,\n",
       "       -1.5636548 , -7.2357607 ,  3.965913  ,  2.4189513 , -6.638649  ,\n",
       "       -5.1098833 ,  6.3025026 , -6.017789  , -9.443461  ,  3.9130876 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if the model is case-sensitive\n",
    "test_vector_1 = model_word_vectors[\"Die\"]\n",
    "test_vector_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9637588 ,  3.340163  ,  2.6202397 , -0.3631109 ,  2.8870285 ,\n",
       "       -6.0887575 , -1.3381153 , -3.089379  ,  1.0239747 , -2.7498906 ,\n",
       "        2.1360517 , -0.51465225, -0.898215  ,  3.830969  ,  4.102815  ,\n",
       "       -2.3922975 , -9.089986  , -0.8491865 , -3.2739496 ,  4.5140324 ,\n",
       "       -1.664932  ,  1.9908719 , -3.1572967 , -1.22494   , -5.072695  ,\n",
       "        0.1696878 ,  0.8671259 , -6.3541837 , -1.3597887 , -0.68297356,\n",
       "        0.7620054 ,  1.2939987 ,  4.094528  , -7.109388  ,  1.7496419 ,\n",
       "        0.7566529 ,  5.1341224 ,  7.7791677 ,  4.6878214 , -0.8463696 ,\n",
       "        3.348159  , -1.1573505 ,  0.30039543, -0.6708167 , -0.9934627 ,\n",
       "        3.9892833 ,  0.46180266,  6.1970863 , -4.85614   , -3.649036  ,\n",
       "       -6.3312283 , -1.9286971 , -0.21158388, -5.8725734 ,  2.6953738 ,\n",
       "        3.5396297 , -3.1873872 ,  3.7628553 ,  0.5165237 ,  4.5804224 ,\n",
       "       -0.90519804,  1.662768  ,  4.0671263 ,  2.8800857 , -0.37748143,\n",
       "       -2.4882345 ,  6.057412  , -4.5931463 , -2.4023619 , -1.646058  ,\n",
       "        3.5106823 ,  3.3382735 , -3.7187033 ,  6.637617  ,  0.44551834,\n",
       "        2.681651  , -5.830269  , -0.08578981, -3.719765  ,  1.3930879 ,\n",
       "       -4.095593  , -0.7564934 ,  2.863987  ,  2.1915057 ,  4.0104218 ,\n",
       "        1.0792371 ,  2.8333273 ,  0.09514198, -6.4757643 ,  2.309184  ,\n",
       "        5.0782666 , -4.55313   , -1.3921758 , -1.9889054 , -3.0442371 ,\n",
       "       -2.887605  , -3.7470717 ,  4.768773  , -7.9743447 ,  2.137042  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vector_2 = model_word_vectors[\"die\"]\n",
    "test_vector_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'test_vector_1'** and **'test_vector_2'** are different vectorrs. The word **'die'** and the word **'Die'** have been both been computed, this indicates that the model is case-sensitive.\n",
    "\n",
    "It should be considered that it is not right or wrong for a model to be case-sensitive; instead it should be considered what the model will be used for. If there is a use case where it is important to be able to distinguish between word in there lower-case form and their proper-case form, then the model should be case-insensitive. Also, if the model is build using only lower-case words then things like proper nouns would have ot be converted to lower-case before being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a particular word is not part of the model, a KeyError will be throw. For example, 'KeyError: \"word 'Donald Trump' not in vocabulary\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can be done with the word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simple_test(model_word_vectors):\n",
    "    # Family\n",
    "    # seun meisie broer suster\n",
    "    a = \"seun\"\n",
    "    b = \"meisie\"\n",
    "    c = \"broer\"\n",
    "    d = \"suster\"\n",
    "\n",
    "    print(\">> Reasoning with word vectors\")\n",
    "    \n",
    "    try:\n",
    "        result = model_word_vectors.most_similar(positive=[b, c], negative=[a])\n",
    "        most_similar_key, similarity = result[0]\n",
    "        print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {:.4f}>> EXPECTED '{}'\" \\\n",
    "              .format(a, b, c, most_similar_key, similarity, d))\n",
    "    except:\n",
    "        print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {}>> EXPECTED '{}'\" \\\n",
    "              .format(\"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", d))\n",
    "        \n",
    "    # Family\n",
    "    # pa ma vader moeder\n",
    "    a = \"pa\"\n",
    "    b = \"ma\"\n",
    "    c = \"vader\"\n",
    "    d = \"moeder\"\n",
    "    \n",
    "    try:\n",
    "        result = model_word_vectors.most_similar(positive=[b, c], negative=[a])\n",
    "        most_similar_key, similarity = result[0]\n",
    "        print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {:.4f}>> EXPECTED '{}'\" \\\n",
    "              .format(a, b, c, most_similar_key, similarity, d))\n",
    "    except:\n",
    "        print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {}>> EXPECTED '{}'\" \\\n",
    "              .format(\"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", d))\n",
    "        \n",
    "    # Plural\n",
    "    # vader moeder broers susters\n",
    "    a = \"vader\"\n",
    "    b = \"moeder\"\n",
    "    c = \"broers\"\n",
    "    d = \"susters\"\n",
    "    \n",
    "    try:\n",
    "        result = model_word_vectors.most_similar(positive=[b, c], negative=[a])\n",
    "        most_similar_key, similarity = result[0]\n",
    "        print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {:.4f}>> EXPECTED '{}'\" \\\n",
    "              .format(a, b, c, most_similar_key, similarity, d))\n",
    "    except:\n",
    "        print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {}>> EXPECTED '{}'\" \\\n",
    "              .format(\"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", d))\n",
    "        \n",
    "    # Opposites\n",
    "    # gelukkig ongelukkig eerlik oneerlik\n",
    "    a = \"gelukkig\"\n",
    "    b = \"ongelukkig\"\n",
    "    c = \"eerlik\"\n",
    "    d = \"oneerlik\"\n",
    "    \n",
    "    try:\n",
    "        result = model_word_vectors.most_similar(positive=[b, c], negative=[a])\n",
    "        most_similar_key, similarity = result[0]\n",
    "        print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {:.4f}>> EXPECTED '{}'\" \\\n",
    "              .format(a, b, c, most_similar_key, similarity, d))\n",
    "    except:\n",
    "        print(\">> '{}' staan tot '{}', soos '{}' staan tot <<'{}'- {}>> EXPECTED '{}'\" \\\n",
    "              .format(\"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", \"UNDEFINED\", d))\n",
    "        \n",
    "    print()\n",
    "\n",
    "    print(\">> Simlarity between words\")\n",
    "\n",
    "    a = \"vrou\"\n",
    "    b = \"man\"\n",
    "    \n",
    "    try:\n",
    "        similarity = model_word_vectors.similarity(a, b)\n",
    "        print(\">> Similarity between '{}' and '{}' is {:.4f}\".format(a, b, similarity))\n",
    "    except:\n",
    "        print(\">> Similarity between '{}' and '{}' is {}\".format(a, b, \"UNDEFINED\"))\n",
    "        \n",
    "    a = \"broer\"\n",
    "    b = \"seun\"\n",
    "    \n",
    "    try:\n",
    "        similarity = model_word_vectors.similarity(a, b)\n",
    "        print(\">> Similarity between '{}' and '{}' is {:.4f}\".format(a, b, similarity))\n",
    "    except:\n",
    "        print(\">> Similarity between '{}' and '{}' is {}\".format(a, b, \"UNDEFINED\"))\n",
    "        \n",
    "    a = \"pa\"\n",
    "    b = \"man\"\n",
    "    similarity = model_word_vectors.similarity(a, b)\n",
    "    print(\">> Similarity between '{}' and '{}' is {:.4f}\".format(a, b, similarity))\n",
    "\n",
    "    a = \"bewus\"\n",
    "    b = \"onbewus\"\n",
    "    \n",
    "    try:\n",
    "        similarity = model_word_vectors.similarity(a, b)\n",
    "        print(\">> Similarity between '{}' and '{}' is {:.4f}\".format(a, b, similarity))\n",
    "    except:\n",
    "        print(\">> Similarity between '{}' and '{}' is {}\".format(a, b, \"UNDEFINED\"))\n",
    "        \n",
    "    print()\n",
    "\n",
    "    print(\">> Most similar words\")\n",
    "\n",
    "    a = \"oom\"\n",
    "    \n",
    "    try:\n",
    "        similar = model_word_vectors.similar_by_word(a)\n",
    "        print(\">> Most similar to '{}'': {}\".format(a, similar[:3]))\n",
    "    except:\n",
    "        print(\">> Most similar to '{}'': {}\".format(a, \"UNDEFINED\"))\n",
    "        \n",
    "    a = \"tannie\"\n",
    "    \n",
    "    try:\n",
    "        similar = model_word_vectors.similar_by_word(a)\n",
    "        print(\">> Most similar to '{}'': {}\".format(a, similar[:3]))\n",
    "    except:\n",
    "        print(\">> Most similar to '{}'': {}\".format(a, \"UNDEFINED\"))\n",
    "        \n",
    "    a = \"oupa\"\n",
    "    \n",
    "    try:\n",
    "        similar = model_word_vectors.similar_by_word(a)\n",
    "        print(\">> Most similar to '{}'': {}\".format(a, similar[:3]))\n",
    "    except:\n",
    "        print(\">> Most similar to '{}'': {}\".format(a, \"UNDEFINED\"))\n",
    "        \n",
    "    a = \"seun\"\n",
    "    \n",
    "    try:\n",
    "        similar = model_word_vectors.similar_by_word(a)\n",
    "        print(\">> Most similar to '{}'': {}\".format(a, similar[:3]))\n",
    "    except:\n",
    "        print(\">> Most similar to '{}'': {}\".format(a, \"UNDEFINED\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Reasoning with word vectors\n",
      ">> 'seun' staan tot 'meisie', soos 'broer' staan tot <<'vrou'- 0.5705>> EXPECTED 'suster'\n",
      ">> 'pa' staan tot 'ma', soos 'vader' staan tot <<'moeder'- 0.6940>> EXPECTED 'moeder'\n",
      ">> 'vader' staan tot 'moeder', soos 'broers' staan tot <<'dogters'- 0.6842>> EXPECTED 'susters'\n",
      ">> 'gelukkig' staan tot 'ongelukkig', soos 'eerlik' staan tot <<'handeling'- 0.4491>> EXPECTED 'oneerlik'\n",
      "\n",
      ">> Simlarity between words\n",
      ">> Similarity between 'vrou' and 'man' is 0.5491\n",
      ">> Similarity between 'broer' and 'seun' is 0.7525\n",
      ">> Similarity between 'pa' and 'man' is 0.4007\n",
      ">> Similarity between 'bewus' and 'onbewus' is 0.5069\n",
      "\n",
      ">> Most similar words\n",
      ">> Most similar to 'oom'': [('broer', 0.7135692238807678), ('dogter', 0.6887587904930115), ('vader', 0.6787451505661011)]\n",
      ">> Most similar to 'tannie'': [('groothertogin', 0.5186679363250732), ('Nijinsky', 0.48672914505004883), ('Petrowna', 0.481914758682251)]\n",
      ">> Most similar to 'oupa'': [('pa', 0.6433155536651611), ('broer', 0.6229803562164307), ('tante', 0.6226500272750854)]\n",
      ">> Most similar to 'seun'': [('dogter', 0.8658730983734131), ('neef', 0.7544885873794556), ('broer', 0.752475380897522)]\n"
     ]
    }
   ],
   "source": [
    "run_simple_test(model_word_vectors=model_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the few simple tests devised it can be seen that the performance of the model is not very impressive. Under the ‘Reasoning with word vectors’ section, none of the four tests were correct. The similarity values ranged from 0.4 to 0.6. Larger similarity values are better, so perhaps aceptable 9or correct) result could be expected to be seen at values of 0.8 and higher.\n",
    "\n",
    "Two things should be noted about testing a Word2Vec model:\n",
    "1.\tThese few tests do not represent a comprehensive evaluation of the model. These were just used to see if the model development is heading in the right direction. A more comprehensive test could be devised and many such tests, with thousands of individual tests, exist.\n",
    "2.\tEven though a more comprehensive test could be devised, I consider the ultimate test to be an actual application of the model. What this means is that if a classifier is built from some Afrikaans text, then the performance of that classifier should be evaluated using the word vectors. If this performance increases then the word vectors can be consdidered useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to improve these results, another model was trained. This time a pre-processing step was introduced. The text from the HTML was processed using the [NLTK](https://www.nltk.org/) library. Specifically, the 'sent_tokenize' function was used. By scanning some of the sentences (which are used by the Gensim Word2Vec function) by eye, it did appear that the first model used large blocks of text (paragraphs) instead of sentences. The second model appeared to use sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "When inspecting the word (token) counts of the text used for training the word vectors, it was observed that tokens like ‘.’, ‘[‘, ‘(‘ appeared. It might improve the model performance if these are removed from the input to the model development process. Unless of course, these are valuable tokens, that might be needed when using the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Reasoning with word vectors\n",
      ">> 'seun' staan tot 'meisie', soos 'broer' staan tot <<'16-jarige'- 0.5693>> EXPECTED 'suster'\n",
      ">> 'pa' staan tot 'ma', soos 'vader' staan tot <<'moeder'- 0.5485>> EXPECTED 'moeder'\n",
      ">> 'UNDEFINED' staan tot 'UNDEFINED', soos 'UNDEFINED' staan tot <<'UNDEFINED'- UNDEFINED>> EXPECTED 'susters'\n",
      ">> 'UNDEFINED' staan tot 'UNDEFINED', soos 'UNDEFINED' staan tot <<'UNDEFINED'- UNDEFINED>> EXPECTED 'oneerlik'\n",
      "\n",
      ">> Simlarity between words\n",
      ">> Similarity between 'vrou' and 'man' is 0.6617\n",
      ">> Similarity between 'broer' and 'seun' is 0.7156\n",
      ">> Similarity between 'pa' and 'man' is 0.2496\n",
      ">> Similarity between 'bewus' and 'onbewus' is UNDEFINED\n",
      "\n",
      ">> Most similar words\n",
      ">> Most similar to 'oom'': UNDEFINED\n",
      ">> Most similar to 'tannie'': UNDEFINED\n",
      ">> Most similar to 'oupa'': UNDEFINED\n",
      ">> Most similar to 'seun'': [('dogter', 0.7809075117111206), ('broer', 0.7156499028205872), ('moeder', 0.674589991569519)]\n"
     ]
    }
   ],
   "source": [
    "# Worde vector model build using an additional preprocessing step. \n",
    "# HTML was processed by the NLTK library, to try to extract sentences\n",
    "model_word_vectors_by_sentence = gensim.models.KeyedVectors.load(\"word2vec_af_wikipedia_10000_by_sent_speed_test.kv\")\n",
    "run_simple_test(model_word_vectors=model_word_vectors_by_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show that many of the words were not found. But, this is to be expected because this model was only trained on 10,000 Html files. There was one-word 'moeder' in the first set of tests ('Reasoning with word vectors') that was correctly matched. It should be noted that the similarity score is not that high though (0.5485). This indicates that further testing should be reveling with a model which used more Html pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
